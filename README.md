# Quora-Insincere-Questions-Classification

## 0. Executive Summary
Based on the ![Kaggle competition](https://www.kaggle.com/c/quora-insincere-questions-classification), classifies the sincerity of Quora questions using F1 score and a Multinomial Naive Bayes model, with a final F1 score around 0.536.

## 1. Introduction
‘Sincerity’ is an interesting measure. Unlike ‘toxic’ comments or questions – which can largely be identified through the use of particular words, often in reference to minority groups – ‘sincerity’ seems to be a more difficult concept to measure. Much like a rhetorical question, it’s less about the use of particular words and more about context and a human-like cognitive understanding of when a question is asking for information and when it is simply being asked for the sake of division or inflammation. With this in mind, I was drawn to the Kaggle Quora Insincere Questions Classification competition, whose goal is to determine whether a question is ‘sincere’ or not using a dataset of roughly 1.3 million questions from Quora. Despite the fact that this project involves simple binary classification (insincere or sincere), the top leaderboard F1 score was only around 0.7, meaning the task itself was quite challenging.

To solve the problem I utilised nested cross-validation to determine both the most appropriate model and the best hyperparameters for each model. I also included a number of preprocessing techniques and steps in both sklearn’s native library and manual application, however, interestingly the highest-scoring model utilised no preprocessing whatsoever although this did involve a trade-off between precision and recall (for the total F1 score). In addition, I was curious about the nature of an ‘insincere’ question as opposed to a ‘toxic’ question and did perform some rudimentary analysis to better understand the dataset, with the results of this analysis being available in the
‘Implementation’ section of this report. I also analysed the final output of the model (outside the scope of testing) to better understand why it generated false positives and false negatives to improve performance of similar models in the future.

## 2. Implementation
There were a number of factors to consider when approaching this problem, including high-level analysis of the dataset to better understand it, the scoring metric used, and algorithm selection and dataset split.

### 2.1 - Analysis
At a high level, the dataset contains 1,302,122 questions, with sincere questions making up 1,225,312 million (93.8%) and insincere making up the remaining 80,810 (6.2%). These are fairly imbalanced classes, however, as they’re both quite large I believed there would be sufficient examples of both for the model to make a useful decision regarding whether an example was sincere or insincere (as opposed to if there were only, say, 100 examples of insincere questions, in which case I would have had to find a solution to balance the datasets).

At the outset, my hypothesis was that unlike a toxic question, an 'insincere' question is harder to spot likely because it contains fewer of the same terms. Interestingly this seemed to be incorrect: traditional toxic dogwhistles including racist and sexist language appear relatively frequently in this dataset, with, for example, the word 'trump' appearing in 7.6% of all insincere samples but only 0.6% of all sincere samples. Figure 1 (next page) shows that the majority of the 40 most common words in the insincere dataset (with some stopwords removed) appear far more frequently in the insincere than the sincere dataset, with notable exceptions being for words that don’t necessarily have negative connotations in isolation (e.g., one, cant, feel, know, get). This made me very curious as to how the model would perform, as there would likely be some issues when some terms appear much more frequently in one class than another.

#### Figure 1: 40 most frequent words in insincere dataset as a proportion of insincere and sincere datasets
<img src="https://github.com/sklavoug/Quora-Insincere-Questions-Classification/blob/main/2.1.png" alt="Figure 1: 40 most frequent words in insincere dataset as a proportion of insincere and sincere datasets" width="1000"/>

### 2.2 - Scoring
The scoring metric for this challenge was F1 score, which is the harmonic mean of precision (TP / (TP + FP)) and recall (TP / (TP + FN)). This is an appropriate measure for an NLP task relating to toxicity, as the precision will punish a model which is too assertive and uses a particular batch of words which appear commonly in toxic comments to make its decision, while the recall will punish a model which is too conservative in its judgements, incorrectly labelling true values as false. Given how many words appeared frequently in the insincere class but infrequently in the sincere class, I suspected any classification model would perform better on recall than precision, as it would be much more likely to incorrectly label a sincere example as insincere because it contained a certain word (e.g., ‘trump’) than to incorrectly label an insincere example as sincere for not containing words.

### 2.3 - Algorithm Selection and Dataset Split
The main three algorithms I compared are logistic regression, support vector machines, and multinomial Naïve-Bayes, as they were noted as being ![good baseline models](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) for text classification and all have existing implementations in sklearn (see Appendix Table 4 for a description).

As this Kaggle competition’s test set did not include the true values for each document, I decided instead to split the training set into a train and test set so I could benchmark my model’s performance. The main consideration in this case would be ensuring that the split favoured the training data (so the model could learn as much as possible) without having too few examples in the test set, however, as the dataset was quite large I thought it would be appropriate to split it with 90% in the training set and 10% in the test set, and notably ensuring that the sets were stratified so the training and test set would have the same proportion of sincere and insincere examples by using sklearn’s train_test_split function with its ‘stratify’ attribute set to true.

## 3. Experimentation
### 3.1 - Preprocessing
Most Natural Language Processing (NLP) is based on ‘vectorisation’, which essentially involves converting each unique word in the text into a column and having each document as a row, with the row x column value representing either the appearance of that word in the text (binary true/false), or the number of times that word appeared in the text (similar to the difference between Bernoulli NB and Multinomial NB), both of which are options in CountVectorizer’s ‘binary’ parameter. This generally leads to sparse matrices with a large number of columns, however, for the purpose of NLP this much information is unnecessary and can be simplified to improve model performance.

There are four key aspects of vectorisation which can affect model performance: word standardisation, stemming, stopwords, and n-grams. In terms of standardisation, vectorisation requires the data to be relatively uniform – the model will still function on non-uniform data, but its performance will be hampered. A good example of this is punctuation, as when vectorisation occurs the words ‘really’, ‘Really’, ‘REALLY’, ‘REALLY?’ and ‘really?’ will be considered completely different and treated as such, when realistically they are the same word. Fortunately sklearn’s CountVectorizer function splits data on symbols such as question marks, and also has a ‘lowercase’ attribute to set all text to lowercase by default (meaning that the five ‘really’s above will all be treated as the same word).

The second aspect is stemming, which relates closely to standardisation. Similarly to how the five ‘really’s are treated separately, there are also different versions of words which essentially mean the same thing. For example, the words ‘mean’, ‘meant’ and ‘meaning’ all share a common root (or stem) in the word ‘mean’, but much like the ‘really’s, they will be treated as different words. Stemming is a way of removing this excess and unnecessary complexity by stemming each word to its root, so all three variations would just be ‘mean’.

The final two aspects are stopwords and n-grams, both of which occur after the words have been standardised. As noted prior, NLP models use a word’s appearance and/or frequency to determine how to classify a document. There are, however, a number of words which will likely appear in all documents (e.g., ‘the’, ‘a’, ‘for’). These words would appear so frequently in all classes that they add very little to the model’s decision-making and are essentially noise, so can be removed (through the ‘stop_words’ parameter). In line with the recommendation from Kaggle’s ![Basic NLP with NLTK guide](https://www.kaggle.com/alvations/basic-nlp-with-nltk), I used the ![stopwords-json](https://github.com/6/stopwords-json/blob/master/dist/en.json) dataset which is more expansive than either sklearn’s or NLTK’s in-built stopword function.

Finally, n-grams are essentially phrases of length n extracted from the text, e.g., 1-grams are unigrams and represent one word, 2-grams are bigrams and represent two words, etc. These words are extracted from the documents and can be important in determining intent or toxicity, e.g., the words ‘trump’ and ‘sucks’ in isolation could lead the classifier to classify the comment as toxic, however, the combined ‘trump sucks’ may be far less common in toxic comments but more common in non-toxic comments, which would change the classification.

My hypothesis, based on the small amount of NLP work I’ve done in the past, was that each of these would improve the model in some way by reducing the overall pool of words it would have to recognise and enriching the data for each document. Interestingly the opposite seemed to be true – removing standardisation, stemming, and stopword removal actually improved the model’s performance, in some cases significantly.

#### Table 1: Preprocessing Experimentation with MNB Algorithm
|           | All Removed  | Lower + Stop  | Lower      | Stop   | Lower + Stop + Stem  |
| --------- | -----------: | ------------: | ---------: | ------:| -------------------: |
| F1        | **0.5629**   | 0.5472        | 0.5574     | 0.5441 | 0.5531               |
| Precision | 0.4776       | **0.491**     | 0.4665     | 0.4887 | 0.4727               |
| Recall    | 0.6854       | 0.618         | **0.6925** | 0.6138 | 0.6664               |

Table 1 shows the outcome of initial testing with a MNB model, and demonstrates that removing all preprocessing steps and simply vectorising the text with the in-built CountVectorizer (and setting lowercase to false) resulted in a higher F1 score than any of the other preprocessing steps. Interestingly this approach did not generate the highest precision or recall, but was instead a case of having a relatively high recall with a middling precision score, thereby achieving the best harmonic mean between the two.

Also interestingly, the best-performing preprocessing combinations for precision and recall were setting the text to lowercase and removing stopwords, and just removing stopwords respectively, however, both of these approaches yielded very low values of the other score among the combinations (i.e., high precision and low recall, or low precision and high recall). For this reason, preprocessing was experimented with heavily (including reproducing some of CountVectorizer’s attributes so NLTK’s stemming could be applied to the text prior to the models being fit), but ultimately setting lowercase to false yielded the best results. N-grams were the final aspect of preprocessing but in order to better test the effect, n-grams were included as part of the cross-validation.

### 3.2 - Nested Cross-Validation
![Nested cross-validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html) is a technique used to prevent information ‘leakage’. This essentially means that having access to both a training and test set can lead to overfitting through repeated attempts and iterations, i.e., I would train my model on the training set and tune its parameters based on performance on the test set, thereby removing the strict barrier between the two and letting information gained during training ‘leak’ into the test set.

Nested cross-validation reduces the risk of information leakage by repeatedly splitting the training data into train, validate, and test sets and then performing iterations with each option of the hyperparameters provided, thereby tuning the hyperparameters at the same time as the models’ scores are compared.

This is an important step as a particular model may work well on both the training and test sets with all their features, but very poorly with a subset of those features. A good example of this actually occurred during cross-validation, where the SVM model had the highest F1 score of all the models tested, however, its score was far less consistent than the MNB model (which was ultimately used) and its standard deviation was the highest of all (see Table 2). This indicates that the model performed well in particular tests but not others, and lacks the consistency one would want from a well-performing model, as it is important that a model be generalizable (i.e., can get a similar or consistent score regardless of the data it receives).

#### Table 2: Results of Nested Cross-Validation
|     | Highest F1 | Lowest F1 | Avg F1    | Stdev     | Iteration 1 | Iteration 2 | Iteration 3 | Iteration 4 |
| :-- | ---------: | --------: | --------: | --------: | ----------: | ----------: | ----------: | ----------: |
| SVM | **0.588**  | 0.047     | 0.4       | 0.16      | 0.473       | 0.403       | 0.336       | 0.382       |
| MNB | 0.556      | **0.47**  | **0.534** | **0.014** | **0.533**   | **0.534**   | **0.533**   | **0.537**   |
| LOG | 0.499      | 0.001     | 0.336     | 0.157     | 0.296       | 0.291       | 0.222       | 0.353       |

The nested cross-validation I implemented used a 5-fold split for the inner validation and a 3-fold split for the outer validation, using randomised search to determine parameters and then feeding the resulting classifier into sklearn’s cross_val_score function to determine the best score. Notably, grid search was initially used to tune parameters but proved to be extremely costly with such a large dataset, so randomised search was implemented instead to cross-validate with reduced computational expense.

Notably, a term-frequency times inverse document-frequency (tfidf) transformer was considered and experimented with, however, it seemed to reduce the performance of all models significantly (to as low as 0.001 F1 score) so was not implemented as part of nested cross-validation.

Within the MNB cross-validation, some interesting trends emerged. Table 3 shows that the highest- scoring iterations used a max_df near the max (either 0.75 or 1), consistently did not use lowercase, and had a relatively high alpha value (0.1 or 0.01), while the lowest-performing models had 0.5 max_df, used lowercase, and used the lowest alpha value (0.0001).

#### Table 3: Hyperparameters and Resulting F1 Score for MNB Model

